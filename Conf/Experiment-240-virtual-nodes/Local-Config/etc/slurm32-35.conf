#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=cuzco
ControlMachine=cuzco32
MpiParams=ports=12000-12999
#ControlAddr=127.0.0.1
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/tmp
SlurmdSpoolDir=/tmp/slurmd.%n
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/usr/local/slurm251/var/run/slurmctld.pid
SlurmdPidFile=/usr/local/slurm251/var/run/slurmd.%n.pid
ProctrackType=proctrack/cgroup
#ProctrackType=proctrack/linuxproc
#PluginDir=
CacheGroups=0
#FirstJobId=
ReturnToService=1
MaxJobCount=122003
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#EpilogSlurmctld=/usr/local/slurm25/etc/epilog.sh
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=task/cgroup
#TaskPluginParam=cgroups
#TrackWCKey=no
#TreeWidth=50
#TmpFs=/tmp/
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=70
InactiveLimit=0
MinJobAge=2
KillWait=30
Waittime=0
EpilogMsgTime=100
#

OverTimeLimit=3

# SCHEDULING
#SchedulerParameters=defer,bf_interval=60,max_job_bf=10,default_queue_depth=20
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/cons_res
SelectTypeParameters= CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK
FastSchedule=1
#PreemptMode=REQUEUE
#PreemptType=preempt/qos
#GresTypes=gpu
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#PriorityWeightQOS=10000000
#
# LOGGING
SlurmctldDebug=9
SlurmctldLogFile=/tmp/slurmctld.log
SlurmdDebug=9
SlurmdLogFile=/tmp/slurmd.%n.log
#JobCompType=jobcomp/filetxt
#JobCompLoc=/usr/local/slurm25/etc/jobcomp.txt
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=60
AcctGatherEnergyType=acct_gather_energy/ipmi
AcctGatherNodeFreq=30
#
#AccountingStorageEnforce=limits
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStorageLoc=slurmDB
#AccountingStoragePass=kameleon
AccountingStorageUser=slurm
AccountingStoragePort=7013

#

#TopologyPlugin=topology/tree

#DefMemPerCPU=50
#MaxMemPerCPU=1800
#Licenses=lustre*1

# COMPUTE NODES
#NodeName=mordor NodeAddr=127.0.0.1 Gres=gpu:2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 State=IDLE RealMemory=800
#NodeName=mordor NodeAddr=127.0.0.1 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 State=IDLE RealMemory=800
NodeName=cuzco[33-35] Procs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 State=UNKNOWN

#PartitionName=exclusive Nodes=virtual[0-499] Default=YES MaxTime=INFINITE State=UP Priority=10 Shared=Exclusive
#PartitionName=shared Nodes=virtual[0-499] MaxTime=INFINITE State=UP Priority=30

PartitionName=exclusive Nodes=cuzco[33-35] Default=YES MaxTime=INFINITE State=UP Priority=10 Shared=Exclusive
PartitionName=shared Nodes=cuzco[33-35] MaxTime=INFINITE State=UP Priority=10

#NodeName=berlin[47-48] Procs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 State=UNKNOWN
